{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InstructGoose - ðŸš§ WORK IN PROGRESS ðŸš§\n",
    "\n",
    "> Implementation of Reinforcement Learning from Human Feedback (RLHF) from the InstructGPT paper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper: InstructGPT - [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "- In the context of RLHF, how to calculate the $L_t^{V F}(\\theta)$, \n",
    "    + Like it's a function of the PPO agent uses to predict how much reward it gets if generates the sequence?\n",
    "- ~~Does the RL model and the SFT model use the same tokenizer?\n",
    "    Yes~~\n",
    "- ~~I don't know how to returns the logit of the generation model~~\n",
    "- Does the PPO Agent (Language Model) has a value network just like the regular PPO Agent?\n",
    "- I don't understand how to calculate the advantage in PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install instruct-goose\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "I used these resources to implement this\n",
    "\n",
    "- Copied the `load_yaml` function from https://github.com/Dahoas/reward-modeling\n",
    "- Learned how to build a dataset to train reward model: https://wandb.ai/carperai/summarize_RLHF/reports/Implementing-RLHF-Learning-to-Summarize-with-trlX--VmlldzozMzAwODM2\n",
    "- Learned how to add value head in PPO agent: https://github.com/lvwerra/trl\n",
    "- Learned how to calculate the loss of PPO agent: https://github.com/lvwerra/trl/blob/main/trl/trainer/ppo_trainer.py\n",
    "- Learned how to use PPO to train RLHF agent: https://github.com/voidful/TextRL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
