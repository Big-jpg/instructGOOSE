{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Callable\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer: Callable, max_length: int):\n",
    "        \n",
    "        self.chosen = []\n",
    "        self.rejected = []\n",
    "        \n",
    "        for data in tqdm(dataset):\n",
    "            chosen, rejected = data[\"chosen\"], data[\"rejected\"]\n",
    "            chosen_encoding = tokenizer(\n",
    "                chosen,\n",
    "                max_length=max_length, padding=\"max_length\", truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            rejected_encoding = tokenizer(\n",
    "                rejected,\n",
    "                max_length=max_length, padding=\"max_length\", truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            self.chosen.append({\n",
    "                \"input_ids\": chosen_encoding[\"input_ids\"],\n",
    "                \"attention_mask\": chosen_encoding[\"attention_mask\"]\n",
    "            })\n",
    "            self.rejected.append({\n",
    "                \"input_ids\": rejected_encoding[\"input_ids\"],\n",
    "                \"attention_mask\": rejected_encoding[\"attention_mask\"]\n",
    "            })\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.chosen)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.chosen[idx][\"input_ids\"],\\\n",
    "               self.chosen[idx][\"attention_mask\"],\\\n",
    "               self.rejected[idx][\"input_ids\"],\\\n",
    "               self.rejected[idx][\"attention_mask\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PromptDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer: Callable, max_length: int):\n",
    "        \n",
    "        self.prompts = []\n",
    "        \n",
    "        for data in tqdm(dataset):\n",
    "            prompt = data[\"prompt\"]\n",
    "            prompt_encoding = tokenizer(\n",
    "                prompt,\n",
    "                max_length=max_length, padding=\"max_length\", truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            self.prompts.append({\n",
    "                \"input_ids\": prompt_encoding[\"input_ids\"],\n",
    "                \"attention_mask\": prompt_encoding[\"attention_mask\"]\n",
    "            })\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.prompts[idx][\"input_ids\"],\\\n",
    "               self.prompts[idx][\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
