{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/education/DATA/projects/ai/RLHF/instructGOOSE/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from transformers import AutoModel\n",
    "import pytorch_lightning as pl \n",
    "from torchtyping import TensorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, model: Callable):\n",
    "        \"\"\"Initialize the agent.\n",
    "\n",
    "        Args:\n",
    "            n_observations (int): The vocab size\n",
    "            model (Callable): The pre-trained language model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        n_embd = model.config.n_embd\n",
    "\n",
    "        self.policy_network = model        \n",
    "        self.value_network = nn.Sequential(\n",
    "            nn.Linear(n_embd, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def get_value(\n",
    "        self, hidden_state: TensorType[\"batch_size\", \"seq_len\", \"n_embd\"]\n",
    "    ) -> TensorType[\"batch_size\", 1]:\n",
    "        return self.value_network(hidden_state)[:, -1, :]\n",
    "    \n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor\n",
    "    ) -> Tuple[\n",
    "        TensorType[\"batch_size\", \"seq_len\", \"vocab_size\"],\n",
    "        TensorType[\"batch_size\", \"seq_len\", \"vocab_size\"],\n",
    "        TensorType[\"batch_size\", \"seq_len\"],\n",
    "        TensorType[\"batch_size\", 1]\n",
    "    ]:\n",
    "        base_output = self.policy_network(\n",
    "            input_ids, attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        last_hidden_state = base_output.hidden_states[-1]\n",
    "        logits = base_output.logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "        logprobs = probs.log()\n",
    "        \n",
    "        action_dist = Categorical(probs=probs)\n",
    "        entropy = action_dist.entropy()\n",
    "        \n",
    "        # predicted reward value\n",
    "        value = self.value_network(last_hidden_state)[:, -1, :]\n",
    "        \n",
    "        return logits, logprobs, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AgentLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, action_logits, rejected_reward):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned} \\operatorname{objective~}(\\phi)= & E_{(x, y) \\sim D_{\\pi_\\phi^{\\mathrm{RL}}}}\\left[r_\\theta(x, y)-\\beta \\log \\left(\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right)\\right]+ \\\\ & \\gamma E_{x \\sim D_{\\text {pretrain }}}\\left[\\log \\left(\\pi_\\phi^{\\mathrm{RL}}(x)\\right)\\right]\\end{aligned}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AgentObjective(nn.Module):\n",
    "    def __init__(\n",
    "        self, model: Callable, sft_model: Callable, reward_model: Callable,\n",
    "        gamma: float, beta: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.sft_model = sft_model\n",
    "        self.reward_model = reward_model\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, input_ids: TensorType[\"batch\", \"seq_len\", \"n_dim\"], attention_mask):\n",
    "        \n",
    "        model_logits = self.model(input_ids, attention_mask)\n",
    "        # TODO: implement these\n",
    "        model_input_ids = None\n",
    "        model_attention_mask = None\n",
    "        model_dist = F.softmax(model_logits, dim=-1)\n",
    "        \n",
    "        sft_logits = self.sft_model(input_ids, attention_mask)\n",
    "        sft_dist = F.softmax(sft_logits, dim=-1)\n",
    "        \n",
    "        reward_score = self.reward_model(model_input_ids, model_attention_mask)\n",
    "        \n",
    "        ratio = torch.log(model_dist / sft_dist)\n",
    "        \n",
    "        # compute the coherent of the generated text\n",
    "        coherent = torch.log(model_dist)\n",
    "        \n",
    "        objective = (reward_score - self.beta*ratio).mean() + self.gamma * coherent.mean()\n",
    "        \n",
    "        return objective\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LitAgent(pl.LightningModule):\n",
    "    def __init__(self, model: Callable, loss_func: Callable):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
