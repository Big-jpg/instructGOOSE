# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/08b_trainer.ipynb.

# %% auto 0
__all__ = ['RLHFTrainer']

# %% ../nbs/08b_trainer.ipynb 3
import pytorch_lightning as pl 

# %% ../nbs/08b_trainer.ipynb 4
from typing import Callable, Tuple

import torch
from torchtyping import TensorType

# %% ../nbs/08b_trainer.ipynb 6
class RLHFTrainer:
    def __init__(
        self, model: Callable, ref_model: Callable, config
    ):
        self.model = model
        self.ref_model = ref_model
        self.epsilon = config.epsilon
        self.ent_coef = config.ent_coef
        self.vf_coef = config.vf_coef
    
    def loss(
        self,
        old_logprobs: torch.FloatTensor,
        values: TensorType["batch_size"],
        rewards: torch.FloatTensor,
        query: torch.LongTensor,
        response: torch.LongTensor,
        model_input: torch.LongTensor,
    ):
        pass
    
    def loss(
        self,
        action_logprobs, entropy: TensorType["batch_size"], values: TensorType["batch_size"],
        prev_logprobs
    ) -> TensorType["batch_size", 1]:
       
        # ref_probs = F.softmax(ref_logits, dim=-1)
        
        ratio = (action_logprobs - prev_logprobs).exp()
        clipped_ratio = torch.clamp(ratio, min=1-self.epsilon, max=1+self.epsilon)
        
        # TODO: Implement the advantages
        advantages = None
        
        unclipped_pg_loss = ratio * advantages
        clipped_pg_loss = clipped_ratio * advantages
        
        pg_loss = torch.min(unclipped_pg_loss, clipped_pg_loss).mean()
        
        entropy_loss = entropy.mean()
        value_losses = values.mean()
        
        loss = pg_loss - self.ent_coef * entropy_loss + self.vf_coef * value_losses
        
        return loss

    def step(
        self,
        queries: TensorType["batch_size", "seq_len"],
        responses: TensorType["batch_size", "seq_len"],
        rewards: TensorType["batch_size"],
    ):
        output = self.forward_batch(queries, responses)
    
    def forward_batch(
        self,
        queries: TensorType["batch_size", "seq_len"],
        responses: TensorType["batch_size", "seq_len"]
    ) -> Tuple[TensorType["batch_size", ""]]:
        inputs = torch.cat([queries, responses], dim=1)
        
        with torch.no_grad():
            _, logprobs, _, value = self.model(inputs)
            _, ref_logprob, _, _ = self.ref_model(inputs)
            
        return logprobs, ref_logprob, value
    
    def forward(
        self,
        input_ids: TensorType["batch", "seq_len", "n_dim"],
        attention_mask: TensorType["batch", "seq_len"]
    ) -> TensorType["batch", "log_probs"]:
        
        with torch.no_grad():
            # action_logits, action_logprobs, entropy, value
            _, logprobs, entropy, value = self.model(input_ids, attention_mask)
            _, ref_logprob, _, _ = self.ref_model(input_ids, attention_mask)
        
        loss = self.loss(logprobs, entropy, value, ref_logprob)
        
