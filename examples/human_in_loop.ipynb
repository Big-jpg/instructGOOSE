{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/education/DATA/projects/ai/RLHF/instructGOOSE/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from instruct_goose.reward import RewardModel\n",
    "from instruct_goose.dataset import PairDataset\n",
    "from instruct_goose.utils import load_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml(\"../configs/sentiment_config.yml\")\n",
    "reward_checkpoint = config[\"reward_model\"][\"model_path\"]\n",
    "reward_data_path = config[\"reward_data\"][\"data_path\"]\n",
    "batch_size = config[\"reward_data\"][\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Dahoas--rm-static-576a4467763bb58a\n",
      "Found cached dataset parquet (/Users/education/.cache/huggingface/datasets/Dahoas___parquet/Dahoas--rm-static-576a4467763bb58a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 398.49it/s]\n"
     ]
    }
   ],
   "source": [
    "reward_dataset = load_dataset(reward_data_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(reward_checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [12:50, 192.75s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(dataloader)):\n",
      "\u001b[1;32m      2\u001b[0m     chosen_input_ids, chosen_attention_mask, rejected_input_ids, rejected_attention_mask \u001b[39m=\u001b[39m batch\n",
      "\u001b[0;32m----> 4\u001b[0m     chosen_rewards \u001b[39m=\u001b[39m reward_model(chosen_input_ids, chosen_attention_mask)\n",
      "\u001b[1;32m      5\u001b[0m     rejected_rewards \u001b[39m=\u001b[39m reward_model(rejected_input_ids, rejected_attention_mask)\n",
      "\u001b[1;32m      7\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/DATA/projects/ai/RLHF/instructGOOSE/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/DATA/projects/ai/RLHF/instructGOOSE/instruct_goose/reward.py:44\u001b[0m, in \u001b[0;36mRewardModel.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n",
      "\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids: torch\u001b[39m.\u001b[39mTensor, attention_mask: torch\u001b[39m.\u001b[39mTensor):\n",
      "\u001b[0;32m---> 44\u001b[0m     last_hidden_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n",
      "\u001b[1;32m     45\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n",
      "\u001b[1;32m     46\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n",
      "\u001b[1;32m     47\u001b[0m     )\u001b[39m.\u001b[39mlast_hidden_state\n",
      "\u001b[1;32m     49\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(last_hidden_state)\n",
      "\u001b[1;32m     50\u001b[0m     reward_scalar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_head(output)\n",
      "\n",
      "File \u001b[0;32m~/DATA/projects/ai/RLHF/instructGOOSE/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/DATA/projects/ai/RLHF/instructGOOSE/env/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:889\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n",
      "\u001b[1;32m    879\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n",
      "\u001b[1;32m    880\u001b[0m         create_custom_forward(block),\n",
      "\u001b[1;32m    881\u001b[0m         hidden_states,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    886\u001b[0m         encoder_attention_mask,\n",
      "\u001b[1;32m    887\u001b[0m     )\n",
      "\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 889\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n",
      "\u001b[1;32m    890\u001b[0m         hidden_states,\n",
      "\u001b[1;32m    891\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n",
      "\u001b[1;32m    892\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n",
      "\u001b[1;32m    893\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n",
      "\u001b[1;32m    894\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n",
      "\u001b[1;32m    895\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n",
      "\u001b[1;32m    896\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n",
      "\u001b[1;32m    897\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n",
      "\u001b[1;32m    898\u001b[0m     )\n",
      "\u001b[1;32m    900\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;32m    901\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m~/DATA/projects/ai/RLHF/instructGOOSE/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/DATA/projects/ai/RLHF/instructGOOSE/env/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:426\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n",
      "\u001b[1;32m    424\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n",
      "\u001b[1;32m    425\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(hidden_states)\n",
      "\u001b[0;32m--> 426\u001b[0m feed_forward_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n",
      "\u001b[1;32m    427\u001b[0m \u001b[39m# residual connection\u001b[39;00m\n",
      "\u001b[1;32m    428\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m feed_forward_hidden_states\n",
      "\n",
      "File \u001b[0;32m~/DATA/projects/ai/RLHF/instructGOOSE/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/DATA/projects/ai/RLHF/instructGOOSE/env/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:355\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n",
      "\u001b[1;32m    353\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_fc(hidden_states)\n",
      "\u001b[1;32m    354\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(hidden_states)\n",
      "\u001b[0;32m--> 355\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_proj(hidden_states)\n",
      "\u001b[1;32m    356\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n",
      "\u001b[1;32m    357\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "\n",
      "File \u001b[0;32m~/DATA/projects/ai/RLHF/instructGOOSE/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/DATA/projects/ai/RLHF/instructGOOSE/env/lib/python3.10/site-packages/transformers/pytorch_utils.py:112\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n",
      "\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n",
      "\u001b[1;32m    111\u001b[0m     size_out \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf,)\n",
      "\u001b[0;32m--> 112\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49maddmm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, x\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n",
      "\u001b[1;32m    113\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(size_out)\n",
      "\u001b[1;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_reward_dataset, _ = random_split(reward_dataset[\"train\"], [10, len(reward_dataset[\"train\"]) - 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model = RewardModel(reward_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1234.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# pair_dataset = PairDataset(reward_dataset[\"train\"], tokenizer, max_length=1024)\n",
    "pair_dataset = PairDataset(small_reward_dataset, tokenizer, max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[48902,    25,  3966,  ..., 50256, 50256, 50256]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0]]),\n",
       " tensor([[48902,    25,   314,  ..., 50256, 50256, 50256]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(pair_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:13, 14.73s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in tqdm(enumerate(dataloader)):\n",
    "    chosen_input_ids, chosen_attention_mask, rejected_input_ids, rejected_attention_mask = batch\n",
    "    \n",
    "    chosen_rewards = reward_model(chosen_input_ids, chosen_attention_mask)\n",
    "    rejected_rewards = reward_model(rejected_input_ids, rejected_attention_mask)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4260, 8.0676, 6.4001,  ..., 8.4922, 6.5932, 1.5312],\n",
       "        [2.3436, 7.9532, 8.3668,  ..., 9.5768, 9.3080, 8.9846]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_rewards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5312, 8.9846], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_rewards[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "last_token = einops.rearrange(chosen_rewards, \"b n -> (b n)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "hiddens = torch.randn([2, 1, 1024, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1024, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0683])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens[0][0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "last_hidden = rearrange(hiddens, 'b 1 t 1 -> b t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0197,  1.3598, -0.9530,  ...,  0.3503,  0.0706,  0.0683])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": " Error while processing rearrange-reduction pattern \"b 1 t 1 -> b t\".\n Input tensor shape: torch.Size([2, 1, 1024, 1]). Additional info: {'t': 1}.\n Shape mismatch, 1024 != 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/DATA/projects/ai/RLHF/instructGOOSE/env/lib/python3.10/site-packages/einops/einops.py:412\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    411\u001b[0m     recipe \u001b[39m=\u001b[39m _prepare_transformation_recipe(pattern, reduction, axes_lengths\u001b[39m=\u001b[39mhashable_axes_lengths)\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m _apply_recipe(recipe, tensor, reduction_type\u001b[39m=\u001b[39;49mreduction)\n\u001b[1;32m    413\u001b[0m \u001b[39mexcept\u001b[39;00m EinopsError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/DATA/projects/ai/RLHF/instructGOOSE/env/lib/python3.10/site-packages/einops/einops.py:235\u001b[0m, in \u001b[0;36m_apply_recipe\u001b[0;34m(recipe, tensor, reduction_type)\u001b[0m\n\u001b[1;32m    233\u001b[0m backend \u001b[39m=\u001b[39m get_backend(tensor)\n\u001b[1;32m    234\u001b[0m init_shapes, reduced_axes, axes_reordering, added_axes, final_shapes \u001b[39m=\u001b[39m \\\n\u001b[0;32m--> 235\u001b[0m     _reconstruct_from_shape(recipe, backend\u001b[39m.\u001b[39;49mshape(tensor))\n\u001b[1;32m    236\u001b[0m tensor \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mreshape(tensor, init_shapes)\n",
      "File \u001b[0;32m~/DATA/projects/ai/RLHF/instructGOOSE/env/lib/python3.10/site-packages/einops/einops.py:191\u001b[0m, in \u001b[0;36m_reconstruct_from_shape_uncached\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(length, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(known_product, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m length \u001b[39m!=\u001b[39m known_product:\n\u001b[0;32m--> 191\u001b[0m         \u001b[39mraise\u001b[39;00m EinopsError(\u001b[39m'\u001b[39m\u001b[39mShape mismatch, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m != \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(length, known_product))\n\u001b[1;32m    192\u001b[0m \u001b[39m# this is enforced when recipe is created\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39m# elif len(unknown_axes) > 1:\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m#     raise EinopsError(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39m#     )\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mEinopsError\u001b[0m: Shape mismatch, 1024 != 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m last_hidden \u001b[39m=\u001b[39m rearrange(hiddens, \u001b[39m'\u001b[39;49m\u001b[39mb 1 t 1 -> b t\u001b[39;49m\u001b[39m'\u001b[39;49m, t\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/DATA/projects/ai/RLHF/instructGOOSE/env/lib/python3.10/site-packages/einops/einops.py:483\u001b[0m, in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mRearrange can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be applied to an empty list\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    482\u001b[0m     tensor \u001b[39m=\u001b[39m get_backend(tensor[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mstack_on_zeroth_dimension(tensor)\n\u001b[0;32m--> 483\u001b[0m \u001b[39mreturn\u001b[39;00m reduce(cast(Tensor, tensor), pattern, reduction\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrearrange\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49maxes_lengths)\n",
      "File \u001b[0;32m~/DATA/projects/ai/RLHF/instructGOOSE/env/lib/python3.10/site-packages/einops/einops.py:420\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    418\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Input is list. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    419\u001b[0m message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mAdditional info: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(axes_lengths)\n\u001b[0;32m--> 420\u001b[0m \u001b[39mraise\u001b[39;00m EinopsError(message \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(e))\n",
      "\u001b[0;31mEinopsError\u001b[0m:  Error while processing rearrange-reduction pattern \"b 1 t 1 -> b t\".\n Input tensor shape: torch.Size([2, 1, 1024, 1]). Additional info: {'t': 1}.\n Shape mismatch, 1024 != 1"
     ]
    }
   ],
   "source": [
    "last_hidden = rearrange(hiddens, 'b 1 t 1 -> b t', t=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f90fd3d87d45ed001cb098ecf86280ef7cabc5a3eba02c8102ea8248767e065"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
